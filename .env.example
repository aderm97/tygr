# TYGR Security Platform Environment Configuration
# Copy this file to .env and configure your settings

# =============================================================================
# LLM Provider Configuration (Required)
# =============================================================================

# Choose your LLM provider and model
# Supported providers: openai, anthropic, ollama, gemini, azure, and more
# See: https://docs.litellm.ai/docs/providers

# OpenAI (Recommended for best results)
TYGR_LLM=openai/gpt-4o
LLM_API_KEY=sk-your-openai-api-key-here

# Anthropic Claude (Alternative)
# TYGR_LLM=anthropic/claude-sonnet-4
# LLM_API_KEY=sk-ant-your-anthropic-key-here

# Local LLM with Ollama (Cost-effective for testing)
# TYGR_LLM=ollama/llama3.1
# LLM_API_BASE=http://localhost:11434
# LLM_API_KEY=not-required-for-ollama

# Google Gemini
# TYGR_LLM=gemini/gemini-1.5-pro
# LLM_API_KEY=your-google-api-key-here

# Azure OpenAI
# TYGR_LLM=azure/gpt-4
# LLM_API_KEY=your-azure-api-key
# LLM_API_BASE=https://your-resource.openai.azure.com

# =============================================================================
# Optional Configuration
# =============================================================================

# Perplexity API for enhanced search capabilities
# PERPLEXITY_API_KEY=your-perplexity-api-key

# Custom API base URL (for proxies or local deployments)
# LLM_API_BASE=http://localhost:8000

# =============================================================================
# Docker Configuration (Advanced)
# =============================================================================

# Docker image tag (default: latest)
# TYGR_DOCKER_IMAGE=tygr-sandbox:latest

# Resource limits (optional, defaults in docker-compose.yml)
# TYGR_MEMORY_LIMIT=8G
# TYGR_CPU_LIMIT=4.0
